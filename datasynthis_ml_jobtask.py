# -*- coding: utf-8 -*-
"""DataSynthis_ML_JobTask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4pv9gx8V_oBY9C0paW6D74V_TL2iBoE
"""

import pandas as pd

#Loading the dataset
data= pd.read_csv("/content/TSLA.csv")
data.head(10)

data.isnull().sum()  # checking for missing values are 0 or not

data.fillna(method='ffill', inplace=True)

# convert date coloumn to datetime

data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)
data.head()

"""**Plotting the data** **bold text**"""

import matplotlib.pyplot as plt

# plotting the stock price

data['Close'].plot(figsize=(10,6), color="orange")
plt.title('Stock Prices Over Time')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.show()

# defining the spiliting ratio

train_size= int(len(data)*0.8)
train = data[:train_size]
test = data[train_size:]

#checking the split

print(f'train size: {len(train)}')
print(f'test size: {len(test)}')

"""**Implementing the model  (ARIMA) => AutoRegressive Integrated Moving Average**"""

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Fit the ARIMA model ()
model = ARIMA(train['Close'], order=(5,1,4))
model_fit = model.fit()

print(model_fit.summary())

forecast = model_fit.forecast(steps=len(test))

plt.figure(figsize=(10,6))
plt.plot(train.index, train['Close'], label="Training Data")
plt.plot(test.index, test['Close'], label = 'Test Data ')
plt.plot(test.index,forecast, label = 'ARIMA Predictions')
plt.title('ARIMA Model Prdictions')
plt.legend()
plt.show()


# Evaluate the model
rmse_arima = np.sqrt(mean_squared_error(test['Close'], forecast))
print(f'RMSE for ARIMA: {rmse_arima}')

"""**LSTM model (Deep Learning)**"""

import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

scaler = MinMaxScaler(feature_range=(0,1))
train_scaled = scaler.fit_transform(train[['Close']])
test_scaled = scaler.transform(test[['Close']])

"""  Preparing data for LSTM   """

def create_dataset(data, time_step=60):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), 0])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 60  # Number of days to look back
X_train, y_train = create_dataset(train_scaled, time_step)
X_test, y_test = create_dataset(test_scaled, time_step)

# Reshape input to be 3D [samples, time steps, features] for LSTM
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

"""Build and Training the LSTM"""

scaler_x = MinMaxScaler()
n_samples, timesteps, n_feats = X_train.shape
X_train_flat = X_train.reshape(n_samples, timesteps * n_feats)
X_train_scaled = scaler_x.fit_transform(X_train_flat).reshape(n_samples, timesteps, n_feats)

scaler_y = MinMaxScaler()
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))

from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.models import Model, Sequential

model = Sequential()
model.add(tf.keras.Input(shape=(timesteps, n_feats)))
model.add(Bidirectional(LSTM(100, return_sequences=True)))
model.add(Dropout(0.25))
model.add(Bidirectional(LSTM(100, return_sequences=True)))
model.add(Dropout(0.25))
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1))

opt = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])

# Callbacks
# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)
# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

history = model.fit(
    X_train_scaled, y_train_scaled,
    validation_split=0.1,
    epochs=20,
    batch_size=32,
    # callbacks=[early_stop, reduce_lr],
    verbose=1
)

X_test_flat = X_test.reshape(X_test.shape[0], timesteps * n_feats)
X_test_scaled = scaler_x.transform(X_test_flat).reshape(X_test.shape[0], timesteps, n_feats)
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

"""Making Prediction"""

# Make predictions
predictions_lstm = model.predict(X_test_scaled)

# Inverse transform to get the actual values
predictions_lstm = scaler_y.inverse_transform(predictions_lstm)
y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1))

# Plot the predictions vs actual values
plt.figure(figsize=(10,6))
plt.plot(test.index[time_step:], y_test_actual, label='Test Data')
plt.plot(test.index[time_step:], predictions_lstm, label='LSTM Predictions')
plt.title('LSTM Model Predictions')
plt.legend()
plt.show()

# Evaluate the model
rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, predictions_lstm))
print(f'RMSE for LSTM: {rmse_lstm}')

accuracy = 100 - (rmse_lstm / np.mean(y_test_actual) * 100)
print("Overall Accuracy: {:.2f}%".format(accuracy))

# Calculate MAPE for ARIMA
mape_arima = np.mean(np.abs((test['Close'] - forecast) / test['Close'])) * 100

# Calculate MAPE for LSTM
mape_lstm = np.mean(np.abs((y_test_actual - predictions_lstm) / y_test_actual)) * 100

# Print the overall accuracy for both models
print(f'MAPE for ARIMA: {mape_arima:.2f}%')
print(f'MAPE for LSTM: {mape_lstm:.2f}%')

"""Tested the values from user input"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

# Function to make future predictions using LSTM
def predict_future(model, scaler, last_data, days):
    # Prepare the input data for prediction (using last 'time_step' data)
    prediction_input = last_data[-60:]  # Use last 60 days for prediction (adjust this based on your time_step)
    prediction_input = prediction_input.reshape(1, -1, 1)  # Reshape for LSTM input

    # List to hold future predictions
    future_predictions = []

    for _ in range(days):
        next_price = model.predict(prediction_input)  # Predict the next day
        future_predictions.append(next_price[0, 0])

        # Update the input with the new predicted value for the next iteration
        next_price_reshaped = next_price.reshape(1, 1, 1)  # Reshape predicted value to match the input shape
        prediction_input = np.append(prediction_input[:, 1:, :], next_price_reshaped, axis=1)

    # Inverse the scaling to get the actual predicted values
    future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))

    return future_predictions

# Function to handle user input and model output
def get_model_output():
    # Get user input for forecast period (number of days)
    forecast_days_input = input("Enter the number of days you want to forecast: ")
    try:
        forecast_days = int(float(forecast_days_input))
    except ValueError:
        print("Invalid input. Please enter an integer.")
        forecast_days = 0 # Set to 0 or handle as appropriate

    if forecast_days > 0:
        # Assume that 'test_scaled' contains the scaled test data from the last 60 days (or train data)
        last_data_point = test_scaled[-60:]  # Use the last 60 days for forecasting

        # Call the function to predict future stock prices
        future_forecast = predict_future(model, scaler, last_data_point, forecast_days)

        # Display the predictions
        print(f"\nForecast for the next {forecast_days} days:")
        for i, prediction in enumerate(future_forecast, 1):
            print(f"Day {i}: {prediction[0]:.2f}")

# Assuming you have your model and scaler already set up, call the function to test it
get_model_output()

"""Rolling Window Evaluation:"""

print("Dataset Length: ", len(data))

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

# Function to create dataset for LSTM (reshape the data into time_steps)
def create_dataset(data, time_step=60):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), 0])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

# Define the LSTM model
model_lstm = tf.keras.Sequential()
model_lstm.add(tf.keras.layers.LSTM(units=50, return_sequences=True, input_shape=(60, 1)))  # Input shape = (time_step, features)
model_lstm.add(tf.keras.layers.LSTM(units=50, return_sequences=False))
model_lstm.add(tf.keras.layers.Dense(units=1))

# Compile the model
model_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Function to perform rolling window evaluation
def rolling_window_evaluation(model, scaler, data, window_size, test_size):
    rmse_scores = []
    mape_scores = []

    global time_step

    for start in range(0, len(data) - window_size - test_size + 1, test_size):

        train_data = data.iloc[start:start + window_size]['Close']
        test_data = data.iloc[start + window_size:start + window_size + test_size]['Close']

        if len(train_data) < time_step or len(test_data) < time_step:
            print(f"Skipping window starting at index {start} due to insufficient data ({len(train_data)} train, {len(test_data)} test).")
            continue

        # Scaling the data
        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))
        test_scaled = scaler.transform(test_data.values.reshape(-1, 1))


        X_train, y_train = create_dataset(train_scaled, time_step)
        X_test, y_test = create_dataset(test_scaled, time_step)


        if len(X_train) == 0 or len(X_test) == 0:
            print(f"Skipping window starting at index {start} as create_dataset returned empty arrays.")
            continue


        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

        # Train the model on the training data
        model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)
        # Make predictions
        predictions = model.predict(X_test)

        # Inverse scale the predictions and actual values
        predictions = scaler.inverse_transform(predictions)
        y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

        # Calculate RMSE and MAPE
        rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))
        mape = mean_absolute_percentage_error(y_test_actual, predictions)

        # Store the scores
        rmse_scores.append(rmse)
        mape_scores.append(mape)
        print(f"Window starting at index {start}: RMSE = {rmse:.2f}, MAPE = {mape:.2f}%")

    # Return the results
    return rmse_scores, mape_scores

time_step = 60
window_size = 120
test_size = 61




print(data.head())

 #Fitting and scaling the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data[['Close']].values)

model_lstm.fit(X_train, y_train, epochs=5, batch_size=32)

# Performance  rolling window evaluation
rmse_scores, mape_scores = rolling_window_evaluation(model_lstm, scaler, data, window_size, test_size)

print(f"RMSE Scores: {rmse_scores}")
print(f"MAPE Scores: {mape_scores}")

"""**Rolling Window Evaluation for ARIMA**"""

pip install statsmodels

import numpy as np
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import MinMaxScaler

# Function to perform rolling window evaluation for ARIMA
def rolling_window_evaluation_arima(data, window_size, test_size):
    rmse_scores = []
    mape_scores = []


    for start in range(0, len(data) - window_size - test_size + 1, test_size):
        # Split the data into training and testing
        train_data = data.iloc[start:start + window_size]['Close']
        test_data = data.iloc[start + window_size:start + window_size + test_size]['Close']

        model = ARIMA(train_data, order=(5,1,0))
        model_fit = model.fit()

        # Make predictions
        forecast = model_fit.forecast(steps=test_size)

        # Calculate RMSE and MAPE for the predictions
        rmse = np.sqrt(mean_squared_error(test_data, forecast))
        mape = mean_absolute_percentage_error(test_data, forecast)


        rmse_scores.append(rmse)
        mape_scores.append(mape)
        print(f"Window starting at index {start}: ARIMA RMSE = {rmse:.2f}, MAPE = {mape:.2f}%")


    return rmse_scores, mape_scores

window_size = 120
test_size = 61

rmse_scores_arima, mape_scores_arima = rolling_window_evaluation_arima(data, window_size, test_size)

print(f"ARIMA RMSE Scores: {rmse_scores_arima}")
print(f"ARIMA MAPE Scores: {mape_scores_arima}")

# Compare LSTM and ARIMA results
print(f"LSTM RMSE Scores: {rmse_scores}")
print(f"LSTM MAPE Scores: {mape_scores}")
print(f"ARIMA RMSE Scores: {rmse_scores_arima}")
print(f"ARIMA MAPE Scores: {mape_scores_arima}")

"""Visualizing the both LSTM & ARIMA After window Rolling"""

import matplotlib.pyplot as plt


plt.figure(figsize=(12, 6))
plt.plot(rmse_scores, marker='o', label='LSTM RMSE')
plt.plot(rmse_scores_arima, marker='o', label='ARIMA RMSE')
plt.title('Rolling Window RMSE Scores')
plt.xlabel('Window Index')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(mape_scores, marker='o', label='LSTM MAPE')
plt.plot(mape_scores_arima, marker='o', label='ARIMA MAPE')
plt.title('Rolling Window MAPE Scores')
plt.xlabel('Window Index')
plt.ylabel('MAPE (%)')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd


window_indices = list(range(len(rmse_scores)))

lstm_scores_dict = {
    'Window Index': window_indices,
    'LSTM RMSE': rmse_scores,
    'LSTM MAPE (%)': [mape * 100 for mape in mape_scores]
}


arima_scores_dict = {
    'Window Index': window_indices,
    'ARIMA RMSE': rmse_scores_arima,
    'ARIMA MAPE (%)': [mape * 100 for mape in mape_scores_arima]
}

lstm_scores_df = pd.DataFrame(lstm_scores_dict)
arima_scores_df = pd.DataFrame(arima_scores_dict)

comparison_df = pd.merge(lstm_scores_df, arima_scores_df, on='Window Index')


print("Performance Comparison Table (Rolling Window Evaluation):")
display(comparison_df)


print("\nAverage Performance across Rolling Windows:")
average_rmse_lstm = np.mean(rmse_scores)
average_mape_lstm = np.mean(mape_scores) * 100
average_rmse_arima = np.mean(rmse_scores_arima)
average_mape_arima = np.mean(mape_scores_arima) * 100

average_scores_dict = {
    'Model': ['LSTM', 'ARIMA'],
    'Average RMSE': [average_rmse_lstm, average_rmse_arima],
    'Average MAPE (%)': [average_mape_lstm, average_mape_arima]
}
average_scores_df = pd.DataFrame(average_scores_dict)
display(average_scores_df)

print("--- Model Accuracy Comparison ---")
print("\nLSTM Model Accuracy:")
print(f"  Before Rolling Window (Initial Test Set):")
print(f"    RMSE: {rmse_lstm:.2f}")
print(f"    Overall Accuracy: {accuracy:.2f}%")
print(f"  After Rolling Window (Average Performance):")
print(f"    Average RMSE: {average_rmse_lstm:.2f}")
print(f"    Average MAPE: {average_mape_lstm:.2f}%")

print("\nARIMA Model Accuracy:")
print(f"  Before Rolling Window (Initial Test Set):")
print(f"    RMSE: {rmse_arima:.2f}")
print(f"    Initial MAPE: (Calculation in cell AoJTrye84l3e resulted in nan)") # Note the issue with initial MAPE
print(f"  After Rolling Window (Average Performance):")
print(f"    Average RMSE: {average_rmse_arima:.2f}")
print(f"    Average MAPE: {average_mape_arima:.2f}%")